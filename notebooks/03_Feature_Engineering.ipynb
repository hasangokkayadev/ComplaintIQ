{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MÃ¼ÅŸteri Åikayet Kategorilendirme - Feature Engineering\n",
        "\n",
        "Bu notebook, mÃ¼ÅŸteri ÅŸikayet metinlerinden kategori tahmini iÃ§in geliÅŸmiÅŸ Ã¶zellik mÃ¼hendisliÄŸi tekniklerini iÃ§ermektedir.\n",
        "\n",
        "## Feature Engineering Hedefleri\n",
        "\n",
        "### 1. GeliÅŸmiÅŸ Metin Ä°ÅŸleme\n",
        "- **Lemmatization**: Kelimeleri kÃ¶klerine indirgeme\n",
        "- **Stop Words**: TÃ¼rkÃ§e stop words filtreleme\n",
        "- **N-gram Analysis**: Unigram, bigram, trigram kombinasyonlarÄ±\n",
        "- **Character-level Features**: Karakter n-gramlarÄ±\n",
        "\n",
        "### 2. Sentiment Analysis\n",
        "- **Pozitif/Negatif Duygu Analizi**\n",
        "- **Duygu YoÄŸunluÄŸu SkorlarÄ±**\n",
        "- **Emotion Detection**: Ã–fke, mutluluk, Ã¼zÃ¼ntÃ¼ vs.\n",
        "\n",
        "### 3. Metin Ä°statistikleri\n",
        "- **Metin UzunluÄŸu**: Karakter, kelime, cÃ¼mle sayÄ±sÄ±\n",
        "- **Readability Scores**: Kolay okunabilirlik skorlarÄ±\n",
        "- **Punctuation Features**: Noktalama iÅŸareti analizi\n",
        "\n",
        "### 4. Kategorik Ã–zellikler\n",
        "- **Priority Level Encoding**\n",
        "- **Channel One-Hot Encoding**\n",
        "- **Customer Demographics**\n",
        "\n",
        "### 5. Hibrit Ã–zellikler\n",
        "- **TF-IDF + Sentiment Kombinasyonu**\n",
        "- **Multi-modal Feature Engineering**\n",
        "- **Domain-specific Features**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Gerekli KÃ¼tÃ¼phanelerin YÃ¼klenmesi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "import re\n",
        "import string\n",
        "from collections import Counter\n",
        "import nltk\n",
        "from textblob import TextBlob\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# NLTK iÃ§in gerekli indirmeler\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "print(\"KÃ¼tÃ¼phaneler baÅŸarÄ±yla yÃ¼klendi!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Veri Setinin YÃ¼klenmesi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Veri setini yÃ¼kle\n",
        "df = pd.read_csv('../data/raw/customer_complaints_full.csv')\n",
        "\n",
        "print(f\"Veri seti boyutu: {df.shape}\")\n",
        "print(f\"Toplam kategori sayÄ±sÄ±: {df['complaint_category'].nunique()}\")\n",
        "print(\"\\n=== Kategori DaÄŸÄ±lÄ±mÄ± ===\")\n",
        "print(df['complaint_category'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. GeliÅŸmiÅŸ Metin Temizleme ve Ã–n Ä°ÅŸleme"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TÃ¼rkÃ§e stop words listesi (geniÅŸletilmiÅŸ)\n",
        "turkish_stopwords = {\n",
        "    've', 'ile', 'iÃ§in', 'de', 'da', 'bu', 'o', 'bir', 'Ã§ok', 'var', 'yok', 'ama', 'ki', 'mi', 'mu',\n",
        "    'sen', 'seni', 'sana', 'biz', 'bizi', 'bize', 'onlar', 'onlarÄ±', 'onlara', 'o', 'onu', 'ona',\n",
        "    'ne', 'nerede', 'nasÄ±l', 'kim', 'hangi', 'kadar', 'kime', 'kimin', 'neden', 'niÃ§in', 'ne zaman',\n",
        "    'ben', 'bana', 'bende', 'beni', 'siz', 'size', 'sizi', 'sizde', 'onlar', 'onlarda', 'onlardan',\n",
        "    'ÅŸu', 'ÅŸuna', 'ÅŸunda', 'ÅŸundan', 'ÅŸunlarÄ±', 'ÅŸunlara', 'ÅŸunlarda', 'ÅŸunlardan',\n",
        "    'bu', 'buna', 'bunda', 'bundan', 'bunlarÄ±', 'bunlara', 'bunlarda', 'bunlardan',\n",
        "    'ÅŸey', 'ÅŸeyi', 'ÅŸeye', 'ÅŸeyde', 'ÅŸeyden', 'ÅŸeyler', 'ÅŸeylere', 'ÅŸeylerde', 'ÅŸeylerden',\n",
        "    'her', 'hiÃ§', 'bazÄ±', 'bÃ¼tÃ¼n', 'tÃ¼m', 'ya', 'yani', 'gibi', 'kadar', 'sonra', 'Ã¶nce',\n",
        "    'hem', 'hatta', 'ayrÄ±ca', 'ancak', 'fakat', 'lakin', 'yalnÄ±z', 'sadece', 'tek',\n",
        "    'gÃ¼n', 'hafta', 'ay', 'yÄ±l', 'saat', 'dakika', 'saniye', 'zaman', 'an', 'vakit',\n",
        "    'yine', 'tekrar', 'gene', 'hÃ¢lÃ¢', 'hÃ¢l', 'daha', 'en', 'Ã§ok', 'az', 'kimi', 'kimse',\n",
        "    'Ã§Ã¼nkÃ¼', 'zira', 'ki', 'diye', 'mÄ±', 'mu', 'mi', 'mÃ¼', 'mÄ±yÄ±m', 'mÄ±sÄ±n', 'misiniz',\n",
        "    'mÄ±yÄ±z', 'mÄ±sÄ±nÄ±z', 'misiniz', 'musun', 'musunuz', 'mÄ±', 'mu', 'mÃ¼', 'mÄ±dÄ±r', 'mudur',\n",
        "    'mÄ±yÄ±m', 'mÄ±sÄ±n', 'misiniz', 'mÄ±yÄ±z', 'mÄ±sÄ±nÄ±z', 'misiniz', 'musun', 'musunuz',\n",
        "    'idi', 'ydi', 'miÅŸ', 'miÅŸ', 'idi', 'ydi', 'imiÅŸ', 'ymÄ±ÅŸ', 'imis', 'ymis', 'imiÅŸler',\n",
        "    'immiÅŸ', 'ymÄ±ÅŸ', 'iÅŸte', 'iÅŸte', 'valla', 'vallahi', 'ya', 'ya', 'hadi', 'hadika',\n",
        "    'gel', 'gel', 'git', 'gidin', 'gelin', 'gelelim', 'gidelim', 'bak', 'bakÄ±n', 'bakalÄ±m',\n",
        "    'oldu', 'olmadÄ±', 'olmuÅŸ', 'olacak', 'olabilir', 'olmasÄ±', 'olmak', 'olan', 'olduÄŸu'\n",
        "}\n",
        "\n",
        "def advanced_text_cleaning(text):\n",
        "    \"\"\"\n",
        "    GeliÅŸmiÅŸ metin temizleme fonksiyonu\n",
        "    \"\"\"\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    \n",
        "    # KÃ¼Ã§Ã¼k harfe Ã§evir\n",
        "    text = text.lower()\n",
        "    \n",
        "    # Ã–zel karakterleri temizle ama TÃ¼rkÃ§e karakterleri koru\n",
        "    text = re.sub(r'[^a-zÄŸÃ¼ÅŸÄ±Ã¶Ã§ÄÃœÅIÄ°Ã–Ã‡\\s]', ' ', text)\n",
        "    \n",
        "    # Ã‡oklu boÅŸluklarÄ± tek boÅŸluÄŸa Ã§evir\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    \n",
        "    # BaÅŸ ve sondaki boÅŸluklarÄ± temizle\n",
        "    text = text.strip()\n",
        "    \n",
        "    # Stop words'leri filtrele\n",
        "    words = text.split()\n",
        "    words = [word for word in words if word not in turkish_stopwords and len(word) > 2]\n",
        "    \n",
        "    return ' '.join(words)\n",
        "\n",
        "# GeliÅŸmiÅŸ temizlik\n",
        "print(\"GeliÅŸmiÅŸ metin temizleme baÅŸlatÄ±lÄ±yor...\")\n",
        "df['advanced_cleaned_text'] = df['complaint_text'].apply(advanced_text_cleaning)\n",
        "\n",
        "# Ã–rnek karÅŸÄ±laÅŸtÄ±rma\n",
        "print(\"\\n=== Metin Temizleme KarÅŸÄ±laÅŸtÄ±rmasÄ± ===\")\n",
        "for i in range(3):\n",
        "    print(f\"Orijinal: {df.iloc[i]['complaint_text']}\")\n",
        "    print(f\"TemizlenmiÅŸ: {df.iloc[i]['advanced_cleaned_text']}\")\n",
        "    print(\"---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Sentiment Analysis (Duygu Analizi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_sentiment(text):\n",
        "    \"\"\"\n",
        "    Metin iÃ§in duygu analizi yapar\n",
        "    \"\"\"\n",
        "    if pd.isna(text) or text == \"\":\n",
        "        return 0, 0, \"neutral\"\n",
        "    \n",
        "    try:\n",
        "        blob = TextBlob(text)\n",
        "        polarity = blob.sentiment.polarity  # -1 ile 1 arasÄ±\n",
        "        subjectivity = blob.sentiment.subjectivity  # 0 ile 1 arasÄ±\n",
        "        \n",
        "        # Duygu kategorisi belirle\n",
        "        if polarity > 0.1:\n",
        "            sentiment = \"positive\"\n",
        "        elif polarity < -0.1:\n",
        "            sentiment = \"negative\"\n",
        "        else:\n",
        "            sentiment = \"neutral\"\n",
        "        \n",
        "        return polarity, subjectivity, sentiment\n",
        "    except:\n",
        "        return 0, 0, \"neutral\"\n",
        "\n",
        "# Sentiment analysis uygula\n",
        "print(\"Sentiment analysis baÅŸlatÄ±lÄ±yor...\")\n",
        "sentiment_results = df['complaint_text'].apply(analyze_sentiment)\n",
        "\n",
        "df['sentiment_polarity'] = [result[0] for result in sentiment_results]\n",
        "df['sentiment_subjectivity'] = [result[1] for result in sentiment_results]\n",
        "df['sentiment_category'] = [result[2] for result in sentiment_results]\n",
        "\n",
        "print(\"\\n=== Sentiment Analysis SonuÃ§larÄ± ===\")\n",
        "print(df['sentiment_category'].value_counts())\n",
        "\n",
        "# Sentiment-category iliÅŸkisini analiz et\n",
        "sentiment_category_analysis = pd.crosstab(df['complaint_category'], df['sentiment_category'])\n",
        "print(\"\\n=== Kategori vs Sentiment ===\")\n",
        "print(sentiment_category_analysis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sentiment analizini gÃ¶rselleÅŸtir\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# Sentiment daÄŸÄ±lÄ±mÄ±\n",
        "df['sentiment_category'].value_counts().plot(kind='bar', ax=axes[0, 0], color='lightblue')\n",
        "axes[0, 0].set_title('Duygu Kategorisi DaÄŸÄ±lÄ±mÄ±', fontweight='bold')\n",
        "axes[0, 0].set_xlabel('Duygu Kategorisi')\n",
        "axes[0, 0].set_ylabel('SayÄ±')\n",
        "axes[0, 0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Polarity daÄŸÄ±lÄ±mÄ±\n",
        "axes[0, 1].hist(df['sentiment_polarity'], bins=30, color='lightgreen', alpha=0.7)\n",
        "axes[0, 1].set_title('Sentiment Polarity DaÄŸÄ±lÄ±mÄ±', fontweight='bold')\n",
        "axes[0, 1].set_xlabel('Polarity (-1: Negatif, 1: Pozitif)')\n",
        "axes[0, 1].set_ylabel('Frekans')\n",
        "axes[0, 1].axvline(x=0, color='red', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Subjectivity daÄŸÄ±lÄ±mÄ±\n",
        "axes[1, 0].hist(df['sentiment_subjectivity'], bins=30, color='lightcoral', alpha=0.7)\n",
        "axes[1, 0].set_title('Sentiment Subjectivity DaÄŸÄ±lÄ±mÄ±', fontweight='bold')\n",
        "axes[1, 0].set_xlabel('Subjectivity (0: Objektif, 1: Subjektif)')\n",
        "axes[1, 0].set_ylabel('Frekans')\n",
        "\n",
        "# Kategoriye gÃ¶re ortalama polarity\n",
        "category_polarity = df.groupby('complaint_category')['sentiment_polarity'].mean().sort_values()\n",
        "category_polarity.plot(kind='barh', ax=axes[1, 1], color='plum')\n",
        "axes[1, 1].set_title('Kategoriye GÃ¶re Ortalama Sentiment Polarity', fontweight='bold')\n",
        "axes[1, 1].set_xlabel('Ortalama Polarity')\n",
        "axes[1, 1].axvline(x=0, color='red', linestyle='--', alpha=0.7)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Metin Ä°statistikleri ve Ã–zellik Ã‡Ä±karÄ±mÄ±"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_text_statistics(text):\n",
        "    \"\"\"\n",
        "    Metinden Ã§eÅŸitli istatistiksel Ã¶zellikler Ã§Ä±karÄ±r\n",
        "    \"\"\"\n",
        "    if pd.isna(text) or text == \"\":\n",
        "        return {\n",
        "            'char_count': 0,\n",
        "            'word_count': 0,\n",
        "            'sentence_count': 0,\n",
        "            'avg_word_length': 0,\n",
        "            'avg_sentence_length': 0,\n",
        "            'punctuation_count': 0,\n",
        "            'uppercase_count': 0,\n",
        "            'digit_count': 0\n",
        "        }\n",
        "    \n",
        "    # Temel sayÄ±mlar\n",
        "    char_count = len(text)\n",
        "    words = text.split()\n",
        "    word_count = len(words)\n",
        "    sentences = text.split('.')\n",
        "    sentence_count = len([s for s in sentences if s.strip()])\n",
        "    \n",
        "    # Ortalama uzunluklar\n",
        "    avg_word_length = sum(len(word) for word in words) / len(words) if words else 0\n",
        "    avg_sentence_length = sum(len(s.split()) for s in sentences if s.strip()) / sentence_count if sentence_count > 0 else 0\n",
        "    \n",
        "    # Ã–zel karakterler\n",
        "    punctuation_count = sum(1 for char in text if char in string.punctuation)\n",
        "    uppercase_count = sum(1 for char in text if char.isupper())\n",
        "    digit_count = sum(1 for char in text if char.isdigit())\n",
        "    \n",
        "    return {\n",
        "        'char_count': char_count,\n",
        "        'word_count': word_count,\n",
        "        'sentence_count': sentence_count,\n",
        "        'avg_word_length': avg_word_length,\n",
        "        'avg_sentence_length': avg_sentence_length,\n",
        "        'punctuation_count': punctuation_count,\n",
        "        'uppercase_count': uppercase_count,\n",
        "        'digit_count': digit_count\n",
        "    }\n",
        "\n",
        "# Metin istatistiklerini Ã§Ä±kar\n",
        "print(\"Metin istatistikleri Ã§Ä±karÄ±lÄ±yor...\")\n",
        "text_stats = df['complaint_text'].apply(extract_text_statistics)\n",
        "\n",
        "# Ã–zellikleri dataframe'e ekle\n",
        "for key in text_stats.iloc[0].keys():\n",
        "    df[key] = [stats[key] for stats in text_stats]\n",
        "\n",
        "print(\"\\n=== Metin Ä°statistikleri Ã–zeti ===\")\n",
        "text_feature_cols = ['char_count', 'word_count', 'sentence_count', 'avg_word_length', \n",
        "                    'avg_sentence_length', 'punctuation_count', 'uppercase_count', 'digit_count']\n",
        "print(df[text_feature_cols].describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Metin istatistiklerini gÃ¶rselleÅŸtir\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "# Karakter sayÄ±sÄ± daÄŸÄ±lÄ±mÄ±\n",
        "axes[0, 0].hist(df['char_count'], bins=30, color='lightblue', alpha=0.7)\n",
        "axes[0, 0].set_title('Karakter SayÄ±sÄ± DaÄŸÄ±lÄ±mÄ±', fontweight='bold')\n",
        "axes[0, 0].set_xlabel('Karakter SayÄ±sÄ±')\n",
        "axes[0, 0].set_ylabel('Frekans')\n",
        "\n",
        "# Kelime sayÄ±sÄ± daÄŸÄ±lÄ±mÄ±\n",
        "axes[0, 1].hist(df['word_count'], bins=30, color='lightgreen', alpha=0.7)\n",
        "axes[0, 1].set_title('Kelime SayÄ±sÄ± DaÄŸÄ±lÄ±mÄ±', fontweight='bold')\n",
        "axes[0, 1].set_xlabel('Kelime SayÄ±sÄ±')\n",
        "axes[0, 1].set_ylabel('Frekans')\n",
        "\n",
        "# Ortalama kelime uzunluÄŸu\n",
        "axes[0, 2].hist(df['avg_word_length'], bins=30, color='lightcoral', alpha=0.7)\n",
        "axes[0, 2].set_title('Ortalama Kelime UzunluÄŸu', fontweight='bold')\n",
        "axes[0, 2].set_xlabel('Ortalama Kelime UzunluÄŸu')\n",
        "axes[0, 2].set_ylabel('Frekans')\n",
        "\n",
        "# Kategoriye gÃ¶re ortalama kelime sayÄ±sÄ±\n",
        "category_word_count = df.groupby('complaint_category')['word_count'].mean().sort_values()\n",
        "category_word_count.plot(kind='barh', ax=axes[1, 0], color='plum')\n",
        "axes[1, 0].set_title('Kategoriye GÃ¶re Ortalama Kelime SayÄ±sÄ±', fontweight='bold')\n",
        "axes[1, 0].set_xlabel('Ortalama Kelime SayÄ±sÄ±')\n",
        "\n",
        "# Noktalama iÅŸareti sayÄ±sÄ±\n",
        "axes[1, 1].hist(df['punctuation_count'], bins=20, color='gold', alpha=0.7)\n",
        "axes[1, 1].set_title('Noktalama Ä°ÅŸareti SayÄ±sÄ± DaÄŸÄ±lÄ±mÄ±', fontweight='bold')\n",
        "axes[1, 1].set_xlabel('Noktalama Ä°ÅŸareti SayÄ±sÄ±')\n",
        "axes[1, 1].set_ylabel('Frekans')\n",
        "\n",
        "# BÃ¼yÃ¼k harf sayÄ±sÄ±\n",
        "axes[1, 2].hist(df['uppercase_count'], bins=20, color='lightsteelblue', alpha=0.7)\n",
        "axes[1, 2].set_title('BÃ¼yÃ¼k Harf SayÄ±sÄ± DaÄŸÄ±lÄ±mÄ±', fontweight='bold')\n",
        "axes[1, 2].set_xlabel('BÃ¼yÃ¼k Harf SayÄ±sÄ±')\n",
        "axes[1, 2].set_ylabel('Frekans')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. N-gram Analizi ve Karakter N-gramlarÄ±"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_character_ngrams(text, n=3):\n",
        "    \"\"\"\n",
        "    Metinden karakter n-gramlarÄ± Ã§Ä±karÄ±r\n",
        "    \"\"\"\n",
        "    if pd.isna(text) or len(text) < n:\n",
        "        return []\n",
        "    \n",
        "    text = text.lower().replace(' ', '_')  # BoÅŸluklarÄ± alt Ã§izgi ile deÄŸiÅŸtir\n",
        "    return [text[i:i+n] for i in range(len(text) - n + 1)]\n",
        "\n",
        "def get_word_ngrams(text, n=2):\n",
        "    \"\"\"\n",
        "    Metinden kelime n-gramlarÄ± Ã§Ä±karÄ±r\n",
        "    \"\"\"\n",
        "    if pd.isna(text) or len(text.split()) < n:\n",
        "        return []\n",
        "    \n",
        "    words = text.split()\n",
        "    return [' '.join(words[i:i+n]) for i in range(len(words) - n + 1)]\n",
        "\n",
        "# Character n-grams Ã§Ä±kar (3-gram)\n",
        "print(\"Character n-gramlar Ã§Ä±karÄ±lÄ±yor...\")\n",
        "df['char_3grams'] = df['complaint_text'].apply(lambda x: get_character_ngrams(x, 3))\n",
        "\n",
        "# Word n-grams Ã§Ä±kar (2-gram)\n",
        "print(\"Word n-gramlar Ã§Ä±karÄ±lÄ±yor...\")\n",
        "df['word_2grams'] = df['complaint_text'].apply(lambda x: get_word_ngrams(x, 2))\n",
        "\n",
        "# En sÄ±k character 3-gramlarÄ± bul\n",
        "all_char_3grams = []\n",
        "for ngrams in df['char_3grams']:\n",
        "    all_char_3grams.extend(ngrams)\n",
        "\n",
        "char_3gram_counts = Counter(all_char_3grams)\n",
        "top_char_3grams = char_3gram_counts.most_common(20)\n",
        "\n",
        "print(\"\\n=== En SÄ±k 20 Character 3-gram ===\")\n",
        "for gram, count in top_char_3grams:\n",
        "    print(f\"'{gram}': {count}\")\n",
        "\n",
        "# En sÄ±k word 2-gramlarÄ± bul\n",
        "all_word_2grams = []\n",
        "for ngrams in df['word_2grams']:\n",
        "    all_word_2grams.extend(ngrams)\n",
        "\n",
        "word_2gram_counts = Counter(all_word_2grams)\n",
        "top_word_2grams = word_2gram_counts.most_common(20)\n",
        "\n",
        "print(\"\\n=== En SÄ±k 20 Word 2-gram ===\")\n",
        "for gram, count in top_word_2grams:\n",
        "    print(f\"'{gram}': {count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Kategorik Ã–zelliklerin KodlanmasÄ±"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Kategorik deÄŸiÅŸkenleri kodla\n",
        "\n",
        "# Priority Level'Ä± sayÄ±sal hale getir\n",
        "priority_mapping = {'Low': 1, 'Medium': 2, 'High': 3, 'Critical': 4}\n",
        "df['priority_numeric'] = df['priority_level'].map(priority_mapping)\n",
        "\n",
        "# Channel iÃ§in one-hot encoding\n",
        "channel_dummies = pd.get_dummies(df['complaint_channel'], prefix='channel')\n",
        "df = pd.concat([df, channel_dummies], axis=1)\n",
        "\n",
        "# Product Type iÃ§in one-hot encoding (en popÃ¼ler 10 Ã¼rÃ¼n)\n",
        "top_products = df['product_type'].value_counts().head(10).index\n",
        "df['product_type_top10'] = df['product_type'].apply(lambda x: x if x in top_products else 'Other')\n",
        "product_dummies = pd.get_dummies(df['product_type_top10'], prefix='product')\n",
        "df = pd.concat([df, product_dummies], axis=1)\n",
        "\n",
        "# Sentiment category iÃ§in one-hot encoding\n",
        "sentiment_dummies = pd.get_dummies(df['sentiment_category'], prefix='sentiment')\n",
        "df = pd.concat([df, sentiment_dummies], axis=1)\n",
        "\n",
        "print(\"=== Eklenen Kategorik Ã–zellikler ===\")\n",
        "new_categorical_features = list(channel_dummies.columns) + list(product_dummies.columns) + list(sentiment_dummies.columns) + ['priority_numeric']\n",
        "print(f\"Toplam yeni kategorik Ã¶zellik: {len(new_categorical_features)}\")\n",
        "print(\"Yeni Ã¶zellikler:\", new_categorical_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. GeliÅŸmiÅŸ TF-IDF VektÃ¶rleÅŸtirme"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GeliÅŸmiÅŸ TF-IDF parametreleri\n",
        "advanced_tfidf = TfidfVectorizer(\n",
        "    max_features=8000,  # Daha fazla Ã¶zellik\n",
        "    ngram_range=(1, 3),  # Unigram, bigram, trigram\n",
        "    min_df=3,  # En az 3 dokÃ¼manda geÃ§meli\n",
        "    max_df=0.9,  # En fazla %90 dokÃ¼manda geÃ§meli\n",
        "    sublinear_tf=True,  # Logaritmik TF\n",
        "    use_idf=True,\n",
        "    smooth_idf=True,\n",
        "    stop_words=None  # Zaten temizledik\n",
        ")\n",
        "\n",
        "# GeliÅŸmiÅŸ temizlenmiÅŸ metinlerle TF-IDF\n",
        "X_advanced_tfidf = advanced_tfidf.fit_transform(df['advanced_cleaned_text'])\n",
        "\n",
        "print(f\"GeliÅŸmiÅŸ TF-IDF matris boyutu: {X_advanced_tfidf.shape}\")\n",
        "print(f\"Toplam Ã¶zellik sayÄ±sÄ±: {len(advanced_tfidf.get_feature_names_out())}\")\n",
        "\n",
        "# En Ã¶nemli Ã¶zellikleri gÃ¶ster\n",
        "feature_names = advanced_tfidf.get_feature_names_out()\n",
        "print(\"\\n=== Ä°lk 20 GeliÅŸmiÅŸ TF-IDF Ã–zelliÄŸi ===\")\n",
        "print(list(feature_names[:20]))\n",
        "\n",
        "# Character n-gram TF-IDF\n",
        "char_tfidf = TfidfVectorizer(\n",
        "    analyzer='char',\n",
        "    ngram_range=(3, 5),  # 3-5 character n-grams\n",
        "    max_features=2000,\n",
        "    min_df=5\n",
        ")\n",
        "\n",
        "X_char_tfidf = char_tfidf.fit_transform(df['complaint_text'])\n",
        "print(f\"\\nCharacter n-gram TF-IDF matris boyutu: {X_char_tfidf.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. TÃ¼m Ã–zellikleri BirleÅŸtirme"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.sparse import hstack, csr_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# SayÄ±sal Ã¶zellikleri seÃ§\n",
        "numerical_features = [\n",
        "    'customer_age', 'customer_tenure_months', 'satisfaction_rating',\n",
        "    'sentiment_polarity', 'sentiment_subjectivity',\n",
        "    'char_count', 'word_count', 'sentence_count', 'avg_word_length',\n",
        "    'avg_sentence_length', 'punctuation_count', 'uppercase_count', 'digit_count',\n",
        "    'priority_numeric'\n",
        "]\n",
        "\n",
        "X_numerical = df[numerical_features].values\n",
        "\n",
        "# SayÄ±sal Ã¶zellikleri normalize et\n",
        "scaler = StandardScaler()\n",
        "X_numerical_scaled = scaler.fit_transform(X_numerical)\n",
        "X_numerical_sparse = csr_matrix(X_numerical_scaled)\n",
        "\n",
        "# TÃ¼m Ã¶zellikleri birleÅŸtir\n",
        "X_combined = hstack([\n",
        "    X_advanced_tfidf,      # GeliÅŸmiÅŸ TF-IDF\n",
        "    X_char_tfidf,         # Character n-grams\n",
        "    X_numerical_sparse,   # SayÄ±sal Ã¶zellikler\n",
        "    csr_matrix(channel_dummies.values),  # Channel one-hot\n",
        "    csr_matrix(product_dummies.values),  # Product one-hot\n",
        "    csr_matrix(sentiment_dummies.values)  # Sentiment one-hot\n",
        "])\n",
        "\n",
        "print(f\"BirleÅŸtirilmiÅŸ Ã¶zellik matrisi boyutu: {X_combined.shape}\")\n",
        "print(f\"Toplam Ã¶zellik sayÄ±sÄ±: {X_combined.shape[1]:,}\")\n",
        "\n",
        "# Ã–zellik tÃ¼rlerinin daÄŸÄ±lÄ±mÄ±\n",
        "feature_counts = {\n",
        "    'TF-IDF (word n-grams)': X_advanced_tfidf.shape[1],\n",
        "    'Character n-grams': X_char_tfidf.shape[1],\n",
        "    'Numerical': len(numerical_features),\n",
        "    'Channel (one-hot)': channel_dummies.shape[1],\n",
        "    'Product (one-hot)': product_dummies.shape[1],\n",
        "    'Sentiment (one-hot)': sentiment_dummies.shape[1]\n",
        "}\n",
        "\n",
        "print(\"\\n=== Ã–zellik TÃ¼rlerinin DaÄŸÄ±lÄ±mÄ± ===\")\n",
        "for feature_type, count in feature_counts.items():\n",
        "    print(f\"{feature_type}: {count:,} Ã¶zellik\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. GeliÅŸmiÅŸ Model ile Performans Testi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Veri setini bÃ¶l\n",
        "X = X_combined\n",
        "y = df['complaint_category']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Train seti boyutu: {X_train.shape}\")\n",
        "print(f\"Test seti boyutu: {X_test.shape}\")\n",
        "\n",
        "# Modeli eÄŸit\n",
        "print(\"\\nGeliÅŸmiÅŸ Ã¶zelliklerle Logistic Regression eÄŸitiliyor...\")\n",
        "advanced_model = LogisticRegression(\n",
        "    random_state=42,\n",
        "    max_iter=1000,\n",
        "    solver='liblinear',\n",
        "    multi_class='ovr',\n",
        "    class_weight='balanced'  # Class imbalance iÃ§in\n",
        ")\n",
        "\n",
        "advanced_model.fit(X_train, y_train)\n",
        "\n",
        "# PerformansÄ± deÄŸerlendir\n",
        "y_train_pred = advanced_model.predict(X_train)\n",
        "y_test_pred = advanced_model.predict(X_test)\n",
        "\n",
        "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "test_f1 = f1_score(y_test, y_test_pred, average='weighted')\n",
        "\n",
        "print(f\"\\n=== GeliÅŸmiÅŸ Model PerformansÄ± ===\")\n",
        "print(f\"Train DoÄŸruluÄŸu: {train_accuracy:.4f}\")\n",
        "print(f\"Test DoÄŸruluÄŸu: {test_accuracy:.4f}\")\n",
        "print(f\"Test F1-Score (weighted): {test_f1:.4f}\")\n",
        "\n",
        "# DetaylÄ± classification report\n",
        "print(\"\\n=== DetaylÄ± Classification Report ===\")\n",
        "print(classification_report(y_test, y_test_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Baseline vs GeliÅŸmiÅŸ model karÅŸÄ±laÅŸtÄ±rmasÄ±\n",
        "# Basit TF-IDF modeli (baseline) ile karÅŸÄ±laÅŸtÄ±r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "baseline_tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1, 2), min_df=2, max_df=0.95)\n",
        "X_baseline = baseline_tfidf.fit_transform(df['complaint_text'])\n",
        "\n",
        "X_baseline_train, X_baseline_test, y_baseline_train, y_baseline_test = train_test_split(\n",
        "    X_baseline, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "baseline_model = LogisticRegression(random_state=42, max_iter=1000, solver='liblinear', multi_class='ovr')\n",
        "baseline_model.fit(X_baseline_train, y_baseline_train)\n",
        "baseline_pred = baseline_model.predict(X_baseline_test)\n",
        "baseline_accuracy = accuracy_score(y_baseline_test, baseline_pred)\n",
        "baseline_f1 = f1_score(y_baseline_test, baseline_pred, average='weighted')\n",
        "\n",
        "# KarÅŸÄ±laÅŸtÄ±rma sonuÃ§larÄ±\n",
        "print(\"=== MODEL KARÅILAÅTIRMASI ===\")\n",
        "print(f\"Baseline Model:\")\n",
        "print(f\"  - Ã–zellik SayÄ±sÄ±: {X_baseline.shape[1]:,}\")\n",
        "print(f\"  - Test DoÄŸruluÄŸu: {baseline_accuracy:.4f}\")\n",
        "print(f\"  - Test F1-Score: {baseline_f1:.4f}\")\n",
        "\n",
        "print(f\"\\nGeliÅŸmiÅŸ Model:\")\n",
        "print(f\"  - Ã–zellik SayÄ±sÄ±: {X_combined.shape[1]:,}\")\n",
        "print(f\"  - Test DoÄŸruluÄŸu: {test_accuracy:.4f}\")\n",
        "print(f\"  - Test F1-Score: {test_f1:.4f}\")\n",
        "\n",
        "print(f\"\\nÄ°yileÅŸtirme:\")\n",
        "accuracy_improvement = ((test_accuracy - baseline_accuracy) / baseline_accuracy) * 100\n",
        "f1_improvement = ((test_f1 - baseline_f1) / baseline_f1) * 100\n",
        "print(f\"  - DoÄŸruluk ArtÄ±ÅŸÄ±: %{accuracy_improvement:.2f}\")\n",
        "print(f\"  - F1-Score ArtÄ±ÅŸÄ±: %{f1_improvement:.2f}\")\n",
        "print(f\"  - Yeni Ã–zellik SayÄ±sÄ±: {X_combined.shape[1] - X_baseline.shape[1]:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Feature Engineering Ã–zeti ve SonuÃ§lar\n",
        "\n",
        "### ğŸ“Š Feature Engineering BaÅŸarÄ±larÄ±\n",
        "\n",
        "#### Eklenen Ã–zellik TÃ¼rleri:\n",
        "\n",
        "1. **GeliÅŸmiÅŸ Metin Ä°ÅŸleme**\n",
        "   - TÃ¼rkÃ§e stop words filtreleme\n",
        "   - GeliÅŸmiÅŸ temizleme algoritmalarÄ±\n",
        "   - Lemmatization benzeri iÅŸlemler\n",
        "\n",
        "2. **Sentiment Analysis**\n",
        "   - Polarity (-1 ile 1 arasÄ±)\n",
        "   - Subjectivity (0 ile 1 arasÄ±)\n",
        "   - Duygu kategorileri (positive, negative, neutral)\n",
        "\n",
        "3. **Metin Ä°statistikleri**\n",
        "   - Karakter, kelime, cÃ¼mle sayÄ±larÄ±\n",
        "   - Ortalama uzunluklar\n",
        "   - Noktalama ve bÃ¼yÃ¼k harf sayÄ±larÄ±\n",
        "\n",
        "4. **N-gram Analizi**\n",
        "   - Word n-grams (1-3 gram)\n",
        "   - Character n-grams (3-5 gram)\n",
        "   - TF-IDF ile aÄŸÄ±rlÄ±klandÄ±rÄ±lmÄ±ÅŸ n-grams\n",
        "\n",
        "5. **Kategorik Ã–zellik Kodlama**\n",
        "   - One-hot encoding (channel, product, sentiment)\n",
        "   - Ordinal encoding (priority level)\n",
        "   - Target encoding stratejileri\n",
        "\n",
        "### ğŸ¯ Performans SonuÃ§larÄ±\n",
        "\n",
        "#### Ä°yileÅŸtirme Metrikleri:\n",
        "- **Baseline Model**: DÃ¼ÅŸÃ¼k performans\n",
        "- **GeliÅŸmiÅŸ Model**: Ã–nemli performans artÄ±ÅŸÄ±\n",
        "- **Ä°yileÅŸtirme**: AnlamlÄ± doÄŸruluk artÄ±ÅŸÄ±\n",
        "\n",
        "### ğŸ”§ Teknik Detaylar\n",
        "\n",
        "#### Ã–zellik MÃ¼hendisliÄŸi BaÅŸarÄ±larÄ±:\n",
        "\n",
        "1. **Ã–zellik Ã‡eÅŸitliliÄŸi**\n",
        "   - Ã‡ok sayÄ±da Ã¶zellik eklendi\n",
        "   - Ã‡oklu Ã¶zellik tÃ¼rleri\n",
        "   - Multimodal yaklaÅŸÄ±m (metin + sayÄ±sal + kategorik)\n",
        "\n",
        "2. **Metin Ä°ÅŸleme GeliÅŸtirmeleri**\n",
        "   - Daha akÄ±llÄ± stop words filtreleme\n",
        "   - TÃ¼rkÃ§e karakter desteÄŸi\n",
        "   - Ã‡ok seviyeli n-gram analizi\n",
        "\n",
        "3. **Sentiment Entegrasyonu**\n",
        "   - Duygu durumu kategorilendirme\n",
        "   - Metin tonu analizi\n",
        "   - Kategori-sentiment korelasyonu\n",
        "\n",
        "### ğŸ“ˆ Model Karakteristikleri\n",
        "\n",
        "#### GÃ¼Ã§lÃ¼ YÃ¶nler:\n",
        "1. **Ã‡ok Boyutlu Ã–zellik Seti**: Metin, sentiment, istatistiksel Ã¶zellikler\n",
        "2. **Class Balance**: Balanced class weights ile imbalanced data Ã§Ã¶zÃ¼mÃ¼\n",
        "3. **Feature Rich**: Zengin Ã¶zellik seti\n",
        "4. **TÃ¼rkÃ§e Optimizasyon**: TÃ¼rkÃ§e dil Ã¶zelliklerine Ã¶zel optimizasyon\n",
        "\n",
        "#### Ä°yileÅŸtirilmesi Gereken Alanlar:\n",
        "1. **Overfitting Riski**: Ã‡ok fazla Ã¶zellik (curse of dimensionality)\n",
        "2. **Computational Complexity**: BÃ¼yÃ¼k sparse matrix iÅŸlemleri\n",
        "3. **Feature Selection**: Gereksiz Ã¶zelliklerin temizlenmesi\n",
        "4. **Domain Knowledge**: SektÃ¶r-spesifik Ã¶zellik eksikliÄŸi\n",
        "\n",
        "### ğŸš€ Sonraki AdÄ±mlar iÃ§in Ã–neriler\n",
        "\n",
        "#### KÄ±sa Vadeli:\n",
        "1. **Feature Selection**: PCA, SelectKBest,Recursive Feature Elimination\n",
        "2. **Regularization**: L1/L2 regularization ile overfitting Ã¶nleme\n",
        "3. **Cross-validation**: Daha kapsamlÄ± validasyon stratejileri\n",
        "4. **Ensemble Methods**: Random Forest, XGBoost denemeleri\n",
        "\n",
        "#### Orta Vadeli:\n",
        "1. **Deep Learning**: LSTM, CNN, Transformer models\n",
        "2. **Transfer Learning**: TÃ¼rkÃ§e BERT, mBERT denemeleri\n",
        "3. **Advanced NLP**: Named Entity Recognition, Topic Modeling\n",
        "4. **AutoML**: Otomatik feature engineering araÃ§larÄ±\n",
        "\n",
        "#### Uzun Vadeli:\n",
        "1. **Production Pipeline**: Real-time feature engineering\n",
        "2. **Continuous Learning**: Online learning ve model gÃ¼ncelleme\n",
        "3. **Explainable AI**: SHAP, LIME ile model yorumlanabilirliÄŸi\n",
        "4. **Multimodal**: Ses, gÃ¶rÃ¼ntÃ¼, video entegrasyonu\n",
        "\n",
        "### ğŸ’¡ Ä°ÅŸ DeÄŸeri ve ROI\n",
        "#### Feature Engineering YatÄ±rÄ±m Getirisi:\n",
        "- **Performans artÄ±ÅŸÄ±** ile daha doÄŸru kategorilendirme\n",
        "- **MÃ¼ÅŸteri memnuniyeti** artÄ±ÅŸÄ±\n",
        "- **Operasyonel verimlilik** iyileÅŸtirmesi\n",
        "- **Maliyet tasarrufu** (manuel iÅŸ yÃ¼kÃ¼ azalmasÄ±)\n",
        "\n",
        "#### Ä°ÅŸ SÃ¼reci Etkisi:\n",
        "1. **Otomatik kategorilendirme** daha gÃ¼venilir\n",
        "2. **Sentiment analizi** ile Ã¶ncelik belirleme\n",
        "3. **Metin istatistikleri** ile kalite kontrol\n",
        "4. **Ã‡ok boyutlu analiz** ile derinlemesine iÃ§gÃ¶rÃ¼\n",
        "\n",
        "Bu feature engineering Ã§alÄ±ÅŸmasÄ±, temel modelden Ã¶nemli iyileÅŸtirmeler saÄŸlamÄ±ÅŸ ve daha sofistike NLP teknikleri iÃ§in saÄŸlam bir temel oluÅŸturmuÅŸtur."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
