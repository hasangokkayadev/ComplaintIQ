"""
ML Pipeline - MÃ¼ÅŸteri ÅŸikayet kategorilendirme pipeline'Ä±
TÃ¼rkiye'ye Ã¶zel veri toplama ve iÅŸleme entegrasyonu
"""
import pandas as pd
import numpy as np
import logging
import joblib
import json
from pathlib import Path
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.preprocessing import StandardScaler
from scipy.sparse import hstack, csr_matrix
import re
from typing import Tuple, Dict, Any, List, Optional
from src.config import DATA_PATHS, MODELS_DIR, BUSINESS_RULES

logger = logging.getLogger(__name__)

class TurkeyDataCollector:
    """
    ðŸ‡¹ðŸ‡· TÃ¼rkiye'ye Ã¶zel mÃ¼ÅŸteri ÅŸikayet veri toplama sÄ±nÄ±fÄ±
    """
    
    def __init__(self):
        """Data collector baÅŸlatma"""
        self.collected_data = []
        self.categories = {
            "ÃœrÃ¼n Kalite Sorunu": ["kalite", "bozuk", "Ã§Ã¼rÃ¼k", "hasarlÄ±", "malzeme"],
            "YanlÄ±ÅŸ ÃœrÃ¼n": ["yanlÄ±ÅŸ", "farklÄ±", "baÅŸka", "istedigim"],
            "Eksik ÃœrÃ¼n": ["eksik", "yok", "tam deÄŸil", "parÃ§a"],
            "Kargo Gecikmesi": ["gecikti", "geÃ§", "zaman", "kargo", "teslimat"],
            "Kargo FirmasÄ± Problemi": ["kargo firmasÄ±", "kurye", "daÄŸÄ±tÄ±m"],
            "Ä°ade/DeÄŸiÅŸim Sorunu": ["iade", "deÄŸiÅŸim", "para iadesi"],
            "Ã–deme/Fatura Sorunu": ["fatura", "Ã¶deme", "para", "kart"],
            "MÃ¼ÅŸteri Hizmetleri Sorunu": ["mÃ¼ÅŸteri hizmetleri", "temsilci", "destek"],
            "Paketleme/Ambalaj Problemi": ["paket", "ambalaj", "kutu", "ezik"],
            "ÃœrÃ¼n AÃ§Ä±klamasÄ± YanÄ±ltÄ±cÄ±": ["aÃ§Ä±klama", "fotoÄŸraf", "Ã¶zellik"],
            "Hizmet Kalite Sorunu": ["hizmet", "kalite", "personel"],
            "Teknik/Uygulama Sorunu": ["teknik", "uygulama", "yazÄ±lÄ±m", "hata"]
        }
    
    def categorize_text(self, text: str) -> Tuple[str, float]:
        """Metni otomatik kategorize etme"""
        text_lower = text.lower()
        scores = {}
        
        for category, keywords in self.categories.items():
            score = sum(1 for keyword in keywords if keyword in text_lower)
            scores[category] = score
        
        if max(scores.values()) > 0:
            predicted_category = max(scores, key=scores.get)
            confidence = min(scores[predicted_category] / 3.0, 1.0)
            return predicted_category, confidence
        else:
            return "Bilinmeyen", 0.0
    
    def clean_text(self, text: str) -> str:
        """Metin temizleme"""
        if not text:
            return ""
        
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r'[^\w\s.,!?ÄŸÃ¼ÅŸÄ±Ã¶Ã§ÄžÃœÅžIÄ°Ã–Ã‡]', '', text)
        text = text.strip()
        
        if len(text) < 5:
            return ""
        
        return text
    
    def add_complaint(self, text: str, source: str = "manual", 
                     category: Optional[str] = None) -> Dict[str, Any]:
        """Åžikayet ekleme"""
        cleaned_text = self.clean_text(text)
        if not cleaned_text:
            return None
        
        if category is None:
            category, confidence = self.categorize_text(cleaned_text)
        else:
            confidence = 1.0
        
        complaint = {
            'text': cleaned_text,
            'category': category,
            'confidence': confidence,
            'source': source,
            'date': pd.Timestamp.now().isoformat()
        }
        
        self.collected_data.append(complaint)
        return complaint
    
    def get_dataframe(self) -> pd.DataFrame:
        """Toplanan veriyi DataFrame'e Ã§evir"""
        if not self.collected_data:
            return pd.DataFrame()
        
        df = pd.DataFrame(self.collected_data)
        df = df.drop_duplicates(subset=['text'], keep='first')
        
        logger.info(f"Toplam veri: {len(df)} kayÄ±t")
        logger.info(f"Kategori daÄŸÄ±lÄ±mÄ±:\n{df['category'].value_counts()}")
        
        return df

class ComplaintClassificationPipeline:
    """
    MÃ¼ÅŸteri ÅŸikayet kategorilendirme pipeline'Ä±
    """
    
    def __init__(self):
        """Pipeline bileÅŸenleri"""
        self.tfidf_vectorizer = None
        self.feature_scaler = None
        self.model = None
        self.categories = None
        self.is_trained = False
    
    def load_data(self, data_path: str = None) -> pd.DataFrame:
        """
        Veri setini yÃ¼kler
        
        Args:
            data_path: Veri dosyasÄ± yolu
            
        Returns:
            YÃ¼klenen dataframe
        """
        if data_path is None:
            data_path = DATA_PATHS["complaints_data"]
        
        logger.info(f"Veri yÃ¼kleniyor: {data_path}")
        df = pd.read_csv(data_path)
        
        logger.info(f"Veri boyutu: {df.shape}")
        logger.info(f"Kategoriler: {df['complaint_category'].value_counts().to_dict()}")
        
        return df
    
    def preprocess_text(self, text: str) -> str:
        """
        Metin Ã¶n iÅŸleme
        
        Args:
            text: Ä°ÅŸlenecek metin
            
        Returns:
            TemizlenmiÅŸ metin
        """
        if pd.isna(text) or text == "":
            return ""
        
        # Temel temizleme
        text = text.lower()
        text = re.sub(r'[^a-zÄŸÃ¼ÅŸÄ±Ã¶Ã§ÄžÃœÅžIÄ°Ã–Ã‡\s]', ' ', text)
        text = re.sub(r'\s+', ' ', text)
        
        return text.strip()
    
    def prepare_features(self, df: pd.DataFrame) -> Tuple[Any, pd.Series]:
        """
        Ã–zellik hazÄ±rlama
        
        Args:
            df: Input dataframe
            
        Returns:
            Ã–zellik matrisi ve hedef deÄŸiÅŸken
        """
        logger.info("Ã–zellikler hazÄ±rlanÄ±yor...")
        
        # Metin temizleme
        df['cleaned_text'] = df['complaint_text'].apply(self.preprocess_text)
        
        # Ek Ã¶zellikler
        df['text_length'] = df['complaint_text'].str.len()
        df['word_count'] = df['complaint_text'].str.split().str.len()
        
        # TF-IDF vektÃ¶rleÅŸtirme
        self.tfidf_vectorizer = TfidfVectorizer(
            max_features=5000,
            ngram_range=(1, 2),
            min_df=2,
            max_df=0.95,
            sublinear_tf=True
        )
        
        X_text = self.tfidf_vectorizer.fit_transform(df['cleaned_text'])
        
        # SayÄ±sal Ã¶zellikler
        self.feature_scaler = StandardScaler()
        X_numerical = self.feature_scaler.fit_transform(
            df[['text_length', 'word_count']].values
        )
        
        # Ã–zellikleri birleÅŸtir
        X_combined = hstack([X_text, csr_matrix(X_numerical)])
        y = df['complaint_category']
        
        logger.info(f"Ã–zellik matrisi boyutu: {X_combined.shape}")
        
        return X_combined, y
    
    def train_model(self, X: Any, y: pd.Series) -> Dict[str, Any]:
        """
        Model eÄŸitimi
        
        Args:
            X: Ã–zellik matrisi
            y: Hedef deÄŸiÅŸken
            
        Returns:
            EÄŸitim sonuÃ§larÄ±
        """
        logger.info("Model eÄŸitimi baÅŸlatÄ±lÄ±yor...")
        
        # Train/test split
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        # Model
        self.model = LogisticRegression(
            C=1.0,
            penalty='l2',
            solver='liblinear',
            random_state=42,
            max_iter=1000,
            class_weight='balanced'
        )
        
        # EÄŸitim
        self.model.fit(X_train, y_train)
        self.categories = self.model.classes_
        self.is_trained = True
        
        # Performans deÄŸerlendirme
        y_pred = self.model.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        
        # Cross-validation
        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
        cv_scores = cross_val_score(self.model, X_train, y_train, cv=cv, scoring='accuracy')
        
        results = {
            'test_accuracy': accuracy,
            'cv_mean': cv_scores.mean(),
            'cv_std': cv_scores.std(),
            'classification_report': classification_report(y_test, y_pred),
            'confusion_matrix': confusion_matrix(y_test, y_pred),
            'test_size': len(y_test),
            'train_size': len(y_train)
        }
        
        logger.info(f"Model eÄŸitimi tamamlandÄ±. Test doÄŸruluÄŸu: {accuracy:.4f}")
        
        return results
    
    def save_model(self, output_dir: Path = None) -> Dict[str, str]:
        """
        Model kaydetme
        
        Args:
            output_dir: KayÄ±t dizini
            
        Returns:
            Kaydedilen dosya yollarÄ±
        """
        if output_dir is None:
            output_dir = MODELS_DIR
        
        output_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info("Model kaydediliyor...")
        
        # Model kaydetme
        model_path = output_dir / 'final_model.pkl'
        joblib.dump(self.model, model_path)
        
        # TF-IDF vectorizer kaydetme
        tfidf_path = output_dir / 'tfidf_vectorizer.pkl'
        joblib.dump(self.tfidf_vectorizer, tfidf_path)
        
        # Feature scaler kaydetme
        scaler_path = output_dir / 'feature_scaler.pkl'
        joblib.dump(self.feature_scaler, scaler_path)
        
        # Metadata kaydetme
        metadata = {
            'model_type': 'LogisticRegression',
            'categories': self.categories.tolist(),
            'features': self.tfidf_vectorizer.get_feature_names_out().shape[0] if self.tfidf_vectorizer else 0,
            'training_info': {
                'is_trained': self.is_trained,
                'has_vectorizer': self.tfidf_vectorizer is not None,
                'has_scaler': self.feature_scaler is not None
            }
        }
        
        import json
        metadata_path = output_dir / 'pipeline_metadata.json'
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2)
        
        paths = {
            'model': str(model_path),
            'tfidf': str(tfidf_path),
            'scaler': str(scaler_path),
            'metadata': str(metadata_path)
        }
        
        logger.info(f"Model kaydedildi: {paths}")
        
        return paths
    
    def load_model(self, model_dir: Path = None) -> bool:
        """
        Model yÃ¼kleme
        
        Args:
            model_dir: Model dizini
            
        Returns:
            YÃ¼kleme baÅŸarÄ±sÄ±
        """
        if model_dir is None:
            model_dir = MODELS_DIR
        
        try:
            logger.info("Pipeline modeli yÃ¼kleniyor...")
            
            # Model yÃ¼kleme
            model_path = model_dir / 'final_model.pkl'
            self.model = joblib.load(model_path)
            
            # TF-IDF vectorizer yÃ¼kleme
            tfidf_path = model_dir / 'tfidf_vectorizer.pkl'
            self.tfidf_vectorizer = joblib.load(tfidf_path)
            
            # Feature scaler yÃ¼kleme
            scaler_path = model_dir / 'feature_scaler.pkl'
            self.feature_scaler = joblib.load(scaler_path)
            
            # Metadata yÃ¼kleme
            metadata_path = model_dir / 'pipeline_metadata.json'
            if metadata_path.exists():
                with open(metadata_path, 'r') as f:
                    metadata = json.load(f)
                self.categories = metadata['categories']
            
            self.is_trained = True
            
            logger.info("Pipeline modeli baÅŸarÄ±yla yÃ¼klendi!")
            
            return True
            
        except Exception as e:
            logger.error(f"Model yÃ¼kleme hatasÄ±: {e}")
            return False
    
    def predict(self, text: str) -> Dict[str, Any]:
        """
        Tahmin yapma
        
        Args:
            text: Tahmin edilecek metin
            
        Returns:
            Tahmin sonucu
        """
        if not self.is_trained:
            raise ValueError("Model henÃ¼z eÄŸitilmemiÅŸ!")
        
        # Metin Ã¶n iÅŸleme
        cleaned_text = self.preprocess_text(text)
        
        # Ã–zellik Ã§Ä±karma
        text_features = self.tfidf_vectorizer.transform([cleaned_text])
        text_length = len(text)
        word_count = len(text.split())
        numerical_features = self.feature_scaler.transform([[text_length, word_count]])
        
        # Ã–zellikleri birleÅŸtir
        features = hstack([text_features, csr_matrix(numerical_features)])
        
        # Tahmin
        prediction = self.model.predict(features)[0]
        probabilities = self.model.predict_proba(features)[0]
        
        # SonuÃ§
        result = {
            'prediction': prediction,
            'confidence': float(probabilities.max()),
            'all_probabilities': {
                cat: float(prob) for cat, prob in zip(self.categories, probabilities)
            },
            'text_length': len(text),
            'word_count': word_count
        }
        
        return result
    
    def evaluate_model(self, X_test: Any, y_test: pd.Series) -> Dict[str, Any]:
        """
        Model deÄŸerlendirme
        
        Args:
            X_test: Test Ã¶zellikleri
            y_test: Test hedefi
            
        Returns:
            DeÄŸerlendirme sonuÃ§larÄ±
        """
        if not self.is_trained:
            raise ValueError("Model henÃ¼z eÄŸitilmemiÅŸ!")
        
        y_pred = self.model.predict(X_test)
        y_proba = self.model.predict_proba(X_test)
        
        from sklearn.metrics import precision_score, recall_score, f1_score
        
        results = {
            'accuracy': accuracy_score(y_test, y_pred),
            'precision_weighted': precision_score(y_test, y_pred, average='weighted'),
            'recall_weighted': recall_score(y_test, y_pred, average='weighted'),
            'f1_weighted': f1_score(y_test, y_pred, average='weighted'),
            'classification_report': classification_report(y_test, y_pred),
            'confusion_matrix': confusion_matrix(y_test, y_pred).tolist()
        }
        
        return results
    
    def run_full_pipeline(self, data_path: str = None, save_model: bool = True) -> Dict[str, Any]:
        """
        Tam pipeline Ã§alÄ±ÅŸtÄ±rma
        
        Args:
            data_path: Veri dosyasÄ± yolu
            save_model: Model kaydedilsin mi
            
        Returns:
            Pipeline sonuÃ§larÄ±
        """
        logger.info("Tam pipeline Ã§alÄ±ÅŸtÄ±rÄ±lÄ±yor...")
        
        # 1. Veri yÃ¼kleme
        df = self.load_data(data_path)
        
        # 2. Ã–zellik hazÄ±rlama
        X, y = self.prepare_features(df)
        
        # 3. Model eÄŸitimi
        training_results = self.train_model(X, y)
        
        # 4. Model kaydetme
        saved_paths = {}
        if save_model:
            saved_paths = self.save_model()
        
        # Pipeline Ã¶zeti
        pipeline_summary = {
            'data_shape': df.shape,
            'features_shape': X.shape,
            'categories': self.categories.tolist(),
            'training_results': training_results,
            'saved_paths': saved_paths,
            'status': 'success'
        }
        
        logger.info("Pipeline baÅŸarÄ±yla tamamlandÄ±!")
        
        return pipeline_summary

# Pipeline instance
pipeline = ComplaintClassificationPipeline()
data_collector = TurkeyDataCollector()

def run_pipeline(data_path: str = None, save: bool = True) -> Dict[str, Any]:
    """
    Pipeline Ã§alÄ±ÅŸtÄ±rma wrapper fonksiyonu
    
    Args:
        data_path: Veri dosyasÄ± yolu
        save: Model kaydedilsin mi
        
    Returns:
        Pipeline sonuÃ§larÄ±
    """
    return pipeline.run_full_pipeline(data_path, save)

def expand_categories_9_to_12(text: str, category: str) -> str:
    """
    9 Ä°ngilizce kategoriyi 12 TÃ¼rkÃ§e kategoriye geniÅŸlet

    Args:
        text: Åžikayet metni
        category: Orijinal kategori

    Returns:
        GeniÅŸletilmiÅŸ kategori
    """
    text_lower = text.lower()

    # Delivery Issues mapping
    if category == "Delivery Issues":
        if any(keyword in text_lower for keyword in ["gecik", "geÃ§", "zaman", "teslimat"]):
            return "Kargo Gecikmesi"
        elif any(keyword in text_lower for keyword in ["kurye", "daÄŸÄ±tÄ±m", "kargo firmasÄ±", "ÅŸube", "teslim edilemedi"]):
            return "Kargo FirmasÄ± Problemi"
        else:
            return "Kargo Gecikmesi"  # default

    # Product Quality mapping
    elif category == "Product Quality":
        if any(keyword in text_lower for keyword in ["kalite", "bozuk", "kusur"]):
            return "ÃœrÃ¼n Kalite Sorunu"
        elif any(keyword in text_lower for keyword in ["paket", "ambalaj", "kutu", "ezik"]):
            return "Paketleme/Ambalaj Problemi"
        elif any(keyword in text_lower for keyword in ["aÃ§Ä±klama", "fotoÄŸraf", "yanÄ±ltÄ±cÄ±"]):
            return "ÃœrÃ¼n AÃ§Ä±klamasÄ± YanÄ±ltÄ±cÄ±"
        else:
            return "ÃœrÃ¼n Kalite Sorunu"  # default

    # Direct mappings
    elif category == "Customer Service":
        return "MÃ¼ÅŸteri Hizmetleri Sorunu"
    elif category == "Technical Support":
        return "Teknik/Uygulama Sorunu"
    elif category == "Return/Refund":
        return "Ä°ade/DeÄŸiÅŸim Sorunu"
    elif category == "Billing Issues":
        return "Ã–deme/Fatura Sorunu"
    elif category == "Website Issues":
        return "Teknik/Uygulama Sorunu"
    elif category == "Service Outage":
        return "Hizmet Kalite Sorunu"
    elif category == "Fraud Issues":
        return "Ã–deme/Fatura Sorunu"

    # Default fallback
    return category

def collect_and_train(complaints: List[Dict[str, str]], save: bool = True) -> Dict[str, Any]:
    """
    Åžikayetleri topla ve modeli eÄŸit

    Args:
        complaints: Åžikayet listesi [{'text': '...', 'category': '...'}, ...]
        save: Model kaydedilsin mi

    Returns:
        Pipeline sonuÃ§larÄ±
    """
    logger.info(f"Veri toplama baÅŸlÄ±yor: {len(complaints)} ÅŸikayet")

    # Åžikayetleri collector'a ekle
    for complaint in complaints:
        data_collector.add_complaint(
            text=complaint.get('text', ''),
            source=complaint.get('source', 'manual'),
            category=complaint.get('category')
        )

    # DataFrame'e Ã§evir
    df = data_collector.get_dataframe()

    if df.empty:
        logger.error("HiÃ§ veri toplanamadÄ±!")
        return {'status': 'error', 'message': 'No data collected'}

    # 9â†’12 label geniÅŸletme
    df['category_new'] = df.apply(lambda row: expand_categories_9_to_12(row['text'], row['category']), axis=1)

    # Veriyi kaydet
    output_path = Path(DATA_PATHS.get("complaints_data", "data/raw/complaints.csv"))
    output_path.parent.mkdir(parents=True, exist_ok=True)

    # Kolon adlarÄ±nÄ± rename et
    df_renamed = df.rename(columns={
        'text': 'complaint_text',
        'category': 'complaint_category',
        'category_new': 'complaint_category_new'
    })

    df_renamed.to_csv(output_path, index=False, encoding='utf-8')
    logger.info(f"Veri kaydedildi: {output_path}")

    # Pipeline Ã§alÄ±ÅŸtÄ±r
    return pipeline.run_full_pipeline(str(output_path), save)

def add_complaint_to_dataset(text: str, category: Optional[str] = None, 
                            source: str = "manual") -> Dict[str, Any]:
    """
    Tek bir ÅŸikayet ekle
    
    Args:
        text: Åžikayet metni
        category: Kategori (opsiyonel)
        source: Veri kaynaÄŸÄ±
        
    Returns:
        Eklenen ÅŸikayet
    """
    return data_collector.add_complaint(text, source, category)

def get_collected_data() -> pd.DataFrame:
    """Toplanan tÃ¼m veriyi al"""
    return data_collector.get_dataframe()