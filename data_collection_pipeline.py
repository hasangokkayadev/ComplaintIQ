"""
ğŸ‡¹ğŸ‡· TÃ¼rkiye'ye Ã–zel Veri Toplama Pipeline
ComplaintIQ - GerÃ§ek MÃ¼ÅŸteri Åikayetleri Data Collection

Bu pipeline aÅŸaÄŸÄ±daki kaynaklardan veri toplar:
1. Google Maps Negatif YorumlarÄ±
2. Åikayetvar.com Åikayetleri  
3. Trendyol/Hepsiburada Negatif Yorumlar
4. ChatGPT Sentetik Veri Ãœretimi
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import numpy as np
import re
import json
import time
import random
from datetime import datetime, timedelta
from typing import List, Dict, Tuple, Optional
import logging
from pathlib import Path
import openai
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
import rate_limiter

# Logging ayarÄ±
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class TurkeyComplaintDataCollector:
    """
    TÃ¼rkiye'ye Ã¶zel mÃ¼ÅŸteri ÅŸikayet data toplama sÄ±nÄ±fÄ±
    """
    
    def __init__(self, config_path: str = "config.json"):
        """Data collector baÅŸlatma"""
        self.config = self.load_config(config_path)
        self.collected_data = []
        self.rate_limiter = rate_limiter.RateLimiter(
            calls_per_minute=self.config.get("rate_limit", 30)
        )
        
    def load_config(self, config_path: str) -> Dict:
        """KonfigÃ¼rasyon dosyasÄ± yÃ¼kle"""
        default_config = {
            "rate_limit": 30,
            "max_retries": 3,
            "timeout": 10,
            "user_agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
            "openai_api_key": "",
            "google_maps_api_key": "",
            "proxy_settings": {}
        }
        
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
                return {**default_config, **config}
        except FileNotFoundError:
            logger.warning(f"Config dosyasÄ± bulunamadÄ±: {config_path}. VarsayÄ±lan ayarlar kullanÄ±lÄ±yor.")
            return default_config
    
    def clean_text(self, text: str) -> str:
        """Metin temizleme fonksiyonu"""
        if not text:
            return ""
            
        # Temel temizlik
        text = re.sub(r'\s+', ' ', text)  # Ã‡oklu boÅŸluklarÄ± tek boÅŸluk
        text = re.sub(r'[^\w\s.,!?ÄŸÃ¼ÅŸÄ±Ã¶Ã§ÄÃœÅIÄ°Ã–Ã‡]', '', text)  # Ã–zel karakterleri temizle
        text = text.strip()
        
        # KÄ±sa metinleri filtrele (5 karakterden az)
        if len(text) < 5:
            return ""
            
        return text
    
    def categorize_complaint(self, text: str) -> Tuple[str, float]:
        """
        Metni otomatik kategorize etme
        TÃ¼rkiye'ye Ã¶zel 12 kategori sistemi
        """
        text_lower = text.lower()
        
        # Kategori kurallarÄ± (keyword matching)
        category_rules = {
            "ÃœrÃ¼n Kalite Sorunu": {
                "keywords": ["kalite", "bozuk", "Ã§Ã¼rÃ¼k", "hasarlÄ±", "malzeme", "iÅŸÃ§ilik", "dayanÄ±klÄ±", "kusur"],
                "weight": 1.0
            },
            "YanlÄ±ÅŸ ÃœrÃ¼n": {
                "keywords": ["yanlÄ±ÅŸ", "farklÄ±", "baÅŸka", "istedigim", "sipar", "gelen"],
                "weight": 1.0
            },
            "Eksik ÃœrÃ¼n": {
                "keywords": ["eksik", "yok", "tam deÄŸil", "parÃ§a", "aks", "kutu"],
                "weight": 1.0
            },
            "Kargo Gecikmesi": {
                "keywords": ["gecikti", "geÃ§", "zaman", "kargo", "teslimat", "bekliyorum"],
                "weight": 1.0
            },
            "Kargo FirmasÄ± Problemi": {
                "keywords": ["kargo firmasÄ±", "kurye", "daÄŸÄ±tÄ±m", "lojistik", "firma"],
                "weight": 1.0
            },
            "Ä°ade/DeÄŸiÅŸim Sorunu": {
                "keywords": ["iade", "deÄŸiÅŸim", "para iadesi", "geri gÃ¶nderme", "iÅŸlem"],
                "weight": 1.0
            },
            "Ã–deme/Fatura Sorunu": {
                "keywords": ["fatura", "Ã¶deme", "para", "kart", "faturalandÄ±rma", "tutar"],
                "weight": 1.0
            },
            "MÃ¼ÅŸteri Hizmetleri Sorunu": {
                "keywords": ["mÃ¼ÅŸteri hizmetleri", "temsilci", "telefon", "destek", "yardÄ±m"],
                "weight": 1.0
            },
            "Paketleme/Ambalaj Problemi": {
                "keywords": ["paket", "ambalaj", "kutu", "paketleme", "hasar", "ezik"],
                "weight": 1.0
            },
            "ÃœrÃ¼n AÃ§Ä±klamasÄ± YanÄ±ltÄ±cÄ±": {
                "keywords": ["aÃ§Ä±klama", "fotoÄŸraf", "Ã¶zellik", "yanlÄ±ÅŸ", "farklÄ±", "uymuyor"],
                "weight": 1.0
            },
            "Hizmet Kalite Sorunu": {
                "keywords": ["hizmet", "kalite", "personel", "davranÄ±ÅŸ", "ortam", "iÅŸletme"],
                "weight": 1.0
            },
            "Teknik/Uygulama Sorunu": {
                "keywords": ["teknik", "uygulama", "yazÄ±lÄ±m", "sistem", "hata", "Ã§alÄ±ÅŸmÄ±yor"],
                "weight": 1.0
            }
        }
        
        # Keyword scoring
        scores = {}
        for category, rules in category_rules.items():
            score = 0
            for keyword in rules["keywords"]:
                if keyword in text_lower:
                    score += rules["weight"]
            scores[category] = score
        
        # En yÃ¼ksek skorlu kategoriyi seÃ§
        if max(scores.values()) > 0:
            predicted_category = max(scores, key=scores.get)
            confidence = min(scores[predicted_category] / 3.0, 1.0)  # Normalize confidence
            return predicted_category, confidence
        else:
            return "Bilinmeyen", 0.0
    
    def scrape_google_maps_reviews(self, business_name: str, location: str, max_reviews: int = 100) -> List[Dict]:
        """
        Google Maps'ten negatif yorumlarÄ± toplama
        """
        logger.info(f"Google Maps yorumlarÄ± toplanÄ±yor: {business_name}, {location}")
        
        try:
            # Google Places API kullanÄ±mÄ± (gerÃ§ek implementasyon)
            # Place Search + Place Details + Reviews API
            
            place_id = self._find_place_id(business_name, location)
            if not place_id:
                logger.warning(f"Ä°ÅŸletme bulunamadÄ±: {business_name}")
                return []
            
            reviews = self._fetch_place_reviews(place_id, max_reviews)
            
            # Sadece negatif yorumlarÄ± filtrele (1-2 yÄ±ldÄ±z)
            negative_reviews = []
            for review in reviews:
                if review.get('rating', 0) <= 2:
                    cleaned_text = self.clean_text(review.get('text', ''))
                    if cleaned_text:
                        category, confidence = self.categorize_complaint(cleaned_text)
                        
                        negative_reviews.append({
                            'text': cleaned_text,
                            'category': category,
                            'confidence': confidence,
                            'source': 'google_maps',
                            'rating': review.get('rating', 0),
                            'business_name': business_name,
                            'location': location,
                            'date': review.get('time', ''),
                            'author': review.get('author_name', ''),
                            'raw_data': review
                        })
            
            logger.info(f"Google Maps'ten {len(negative_reviews)} negatif yorum toplandÄ±")
            return negative_reviews
            
        except Exception as e:
            logger.error(f"Google Maps scraping hatasÄ±: {e}")
            return []
    
    def scrape_sikayetvar(self, category: str = None, max_pages: int = 10) -> List[Dict]:
        """
        Åikayetvar.com'dan ÅŸikayetleri toplama
        """
        logger.info("Åikayetvar.com'dan ÅŸikayetler toplanÄ±yor")
        
        complaints = []
        
        try:
            headers = {
                'User-Agent': self.config['user_agent'],
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'tr-TR,tr;q=0.9,en;q=0.8',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive',
            }
            
            for page in range(1, max_pages + 1):
                self.rate_limiter.wait()
                
                url = f"https://www.sikayetvar.com/{category or ''}?page={page}"
                
                try:
                    response = requests.get(url, headers=headers, timeout=self.config['timeout'])
                    response.raise_for_status()
                    
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # Åikayet kartlarÄ±nÄ± bul
                    complaint_cards = soup.find_all('div', class_='complaint-item')
                    
                    for card in complaint_cards:
                        try:
                            text_element = card.find('p', class_='complaint-text')
                            if not text_element:
                                continue
                                
                            cleaned_text = self.clean_text(text_element.get_text())
                            if not cleaned_text:
                                continue
                            
                            category_element = card.find('span', class_='category')
                            detected_category = category_element.get_text().strip() if category_element else "Bilinmeyen"
                            
                            # Otomatik kategorilendirme
                            auto_category, confidence = self.categorize_complaint(cleaned_text)
                            
                            # Tarih bilgisi
                            date_element = card.find('time')
                            date = date_element.get('datetime') if date_element else ""
                            
                            complaints.append({
                                'text': cleaned_text,
                                'category': auto_category,
                                'confidence': confidence,
                                'source': 'sikayetvar',
                                'original_category': detected_category,
                                'date': date,
                                'url': card.find('a')['href'] if card.find('a') else "",
                                'raw_data': str(card)[:500]  # Ä°lk 500 karakter
                            })
                            
                        except Exception as e:
                            logger.warning(f"Åikayet kartÄ± iÅŸlenemedi: {e}")
                            continue
                    
                    logger.info(f"Sayfa {page}: {len(complaints)} ÅŸikayet toplandÄ±")
                    
                except requests.RequestException as e:
                    logger.error(f"Sayfa {page} yÃ¼klenemedi: {e}")
                    continue
            
            logger.info(f"Åikayetvar.com'dan toplam {len(complaints)} ÅŸikayet toplandÄ±")
            return complaints
            
        except Exception as e:
            logger.error(f"Åikayetvar scraping hatasÄ±: {e}")
            return []
    
    def scrape_ecommerce_reviews(self, platform: str = "trendyol", max_products: int = 50) -> List[Dict]:
        """
        Trendyol/Hepsiburada'dan negatif Ã¼rÃ¼n yorumlarÄ±nÄ± toplama
        """
        logger.info(f"{platform.capitalize()}'den negatif yorumlar toplanÄ±yor")
        
        reviews = []
        
        # Ã–rnek Ã¼rÃ¼n kategorileri (gerÃ§ek implementasyonda dinamik olacak)
        categories = ["elektronik", "giyim", "ev-yasam", "kozmetik"]
        
        try:
            for category in categories:
                try:
                    products = self._get_category_products(platform, category, max_products // len(categories))
                    
                    for product in products:
                        self.rate_limiter.wait()
                        product_reviews = self._get_product_reviews(platform, product['id'])
                        
                        # Sadece negatif yorumlarÄ± filtrele
                        for review in product_reviews:
                            if review.get('rating', 0) <= 2:
                                cleaned_text = self.clean_text(review.get('text', ''))
                                if cleaned_text:
                                    category_pred, confidence = self.categorize_complaint(cleaned_text)
                                    
                                    reviews.append({
                                        'text': cleaned_text,
                                        'category': category_pred,
                                        'confidence': confidence,
                                        'source': platform,
                                        'rating': review.get('rating', 0),
                                        'product_name': product['name'],
                                        'product_category': category,
                                        'date': review.get('date', ''),
                                        'author': review.get('author', ''),
                                        'raw_data': review
                                    })
                
                except Exception as e:
                    logger.warning(f"{category} kategorisi iÅŸlenemedi: {e}")
                    continue
            
            logger.info(f"{platform.capitalize()}'den toplam {len(reviews)} negatif yorum toplandÄ±")
            return reviews
            
        except Exception as e:
            logger.error(f"{platform} scraping hatasÄ±: {e}")
            return []
    
    def generate_synthetic_data(self, num_samples: int = 1000) -> List[Dict]:
        """
        ChatGPT API ile sentetik ÅŸikayet verisi Ã¼retme
        """
        logger.info(f"ChatGPT ile {num_samples} sentetik ÅŸikayet Ã¼retiliyor")
        
        if not self.config.get('openai_api_key'):
            logger.warning("OpenAI API key bulunamadÄ±, sentetik veri Ã¼retilemiyor")
            return []
        
        synthetic_data = []
        
        # Her kategori iÃ§in eÅŸit daÄŸÄ±lÄ±m
        categories = list(self.categorize_complaint("dummy").keys())
        samples_per_category = num_samples // len(categories)
        
        openai.api_key = self.config['openai_api_key']
        
        try:
            for category in categories:
                if category == "Bilinmeyen":
                    continue
                
                # Kategori Ã¶rneklemesi iÃ§in template prompt
                prompt = f"""
                TÃ¼rkiye'de {category} konusunda gerÃ§ekÃ§i mÃ¼ÅŸteri ÅŸikayeti yaz.
                Ã–zellikler:
                - GerÃ§ek TÃ¼rkÃ§e dil kullan
                - 10-50 kelime arasÄ± olsun
                - Duygusal ve samimi olsun
                - FarklÄ± varyasyonlarla 5 farklÄ± Ã¶rnek ver
                
                Kategori: {category}
                """
                
                response = openai.ChatCompletion.create(
                    model="gpt-3.5-turbo",
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=500,
                    temperature=0.8
                )
                
                generated_texts = response.choices[0].message.content.split('\n')
                
                for text in generated_texts:
                    cleaned_text = self.clean_text(text)
                    if cleaned_text and len(cleaned_text) > 10:
                        synthetic_data.append({
                            'text': cleaned_text,
                            'category': category,
                            'confidence': 0.9,  # Sentetik veri yÃ¼ksek gÃ¼ven
                            'source': 'synthetic_chatgpt',
                            'generated_by': 'gpt-3.5-turbo',
                            'date': datetime.now().isoformat()
                        })
                
                logger.info(f"{category} kategorisi iÃ§in sentetik veri Ã¼retildi")
                
        except Exception as e:
            logger.error(f"Sentetik veri Ã¼retim hatasÄ±: {e}")
        
        logger.info(f"Toplam {len(synthetic_data)} sentetik veri Ã¼retildi")
        return synthetic_data
    
    def _find_place_id(self, business_name: str, location: str) -> Optional[str]:
        """Google Places API ile iÅŸletme ID bulma"""
        # Place Search API implementasyonu
        pass
    
    def _fetch_place_reviews(self, place_id: str, max_reviews: int) -> List[Dict]:
        """Google Places API ile yorumlarÄ± Ã§ekme"""
        # Place Details + Reviews API implementasyonu
        pass
    
    def _get_category_products(self, platform: str, category: str, limit: int) -> List[Dict]:
        """E-ticaret platform'undan kategori Ã¼rÃ¼nlerini Ã§ekme"""
        # Platform API implementasyonu
        pass
    
    def _get_product_reviews(self, platform: str, product_id: str) -> List[Dict]:
        """E-ticaret platform'undan Ã¼rÃ¼n yorumlarÄ±nÄ± Ã§ekme"""
        # Platform API implementasyonu
        pass
    
    def collect_all_data(self) -> pd.DataFrame:
        """TÃ¼m kaynaklardan veri toplama"""
        logger.info("TÃ¼m veri kaynaklarÄ±ndan toplama baÅŸlÄ±yor...")
        
        all_data = []
        
        # 1. Google Maps (Ã–rnek iÅŸletmeler)
        sample_businesses = [
            ("McDonald's", "Ä°stanbul"),
            ("Starbucks", "Ankara"),
            ("KFC", "Ä°zmir"),
            ("Domino's Pizza", "Bursa"),
            ("CarrefourSA", "Antalya")
        ]
        
        for business, location in sample_businesses:
            google_data = self.scrape_google_maps_reviews(business, location, 20)
            all_data.extend(google_data)
            time.sleep(2)  # Rate limiting
        
        # 2. Åikayetvar
        sikayetvar_data = self.scrape_sikayetvar(max_pages=5)
        all_data.extend(sikayetvar_data)
        
        # 3. E-ticaret yorumlarÄ±
        trendyol_data = self.scrape_ecommerce_reviews("trendyol", 25)
        all_data.extend(trendyol_data)
        
        hepsiburada_data = self.scrape_ecommerce_reviews("hepsiburada", 25)
        all_data.extend(hepsiburada_data)
        
        # 4. Sentetik veri
        synthetic_data = self.generate_synthetic_data(500)
        all_data.extend(synthetic_data)
        
        # DataFrame'e Ã§evir
        df = pd.DataFrame(all_data)
        
        if not df.empty:
            # Duplicate temizleme
            df = df.drop_duplicates(subset=['text'], keep='first')
            
            # Kategori daÄŸÄ±lÄ±mÄ± logla
            logger.info(f"Toplam veri: {len(df)} kayÄ±t")
            logger.info(f"Kategori daÄŸÄ±lÄ±mÄ±:\n{df['category'].value_counts()}")
        
        return df
    
    def save_data(self, df: pd.DataFrame, filename: str = None) -> str:
        """Toplanan veriyi kaydetme"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"turkey_complaints_dataset_{timestamp}.csv"
        
        filepath = Path("data/collected") / filename
        filepath.parent.mkdir(parents=True, exist_ok=True)
        
        df.to_csv(filepath, index=False, encoding='utf-8')
        logger.info(f"Veri kaydedildi: {filepath}")
        
        return str(filepath)

# KullanÄ±m Ã¶rneÄŸi
if __name__ == "__main__":
    # Data collector oluÅŸtur
    collector = TurkeyComplaintDataCollector()
    
    # TÃ¼m veriyi topla
    dataset = collector.collect_all_data()
    
    if not dataset.empty:
        # Veriyi kaydet
        filepath = collector.save_data(dataset)
        
        # Ã–zet istatistikler
        print(f"\nğŸ“Š Veri Toplama Ã–zeti:")
        print(f"Toplam kayÄ±t: {len(dataset)}")
        print(f"Kaynak daÄŸÄ±lÄ±mÄ±:")
        print(dataset['source'].value_counts())
        print(f"\nKategori daÄŸÄ±lÄ±mÄ±:")
        print(dataset['category'].value_counts())
        
        print(f"\nâœ… Dataset kaydedildi: {filepath}")
    else:
        print("âŒ HiÃ§ veri toplanamadÄ±!")
